# -*- coding: utf-8 -*-
"""CNN - Chest_X_Rays.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SwXFOFwm7c7v1_HNzhyqFLF1eWdjCfhI
"""

from google.colab import drive
drive.mount('/content/drive')

# # Make sure to run this cell to use torchmetrics.
!pip install torch torchvision torchmetrics

# Import required libraries
# -------------------------
# Data loading
import random
import numpy as np
from torchvision.transforms import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import random_split, DataLoader
import matplotlib.pyplot as plt

# Train model
import torch
from torchvision import models
import torch.nn as nn
import torch.optim as optim
import torch.nn.init as init

# Evaluate model
from torchmetrics import Accuracy, F1Score, Recall, Precision

# Set random seeds for reproducibility
torch.manual_seed(101010)
np.random.seed(101010)
random.seed(101010)

# Define the transformations to apply to the images.
transform_mean = [0.485, 0.456, 0.406]
transform_std =[0.229, 0.224, 0.225]
train_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.RandomRotation((0,30)),
                                transforms.RandomHorizontalFlip(),
                                transforms.Normalize(mean=transform_mean, std=transform_std)])

val_test_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=transform_mean, std=transform_std)])

# Apply the image transforms
train_dataset = ImageFolder("/content/drive/MyDrive/Colab Notebooks/CNN-Chest_X_Ray/data/chestxrays/train", transform=train_transform)
test_dataset = ImageFolder('/content/drive/MyDrive/Colab Notebooks/CNN-Chest_X_Ray/data/chestxrays/test', transform=val_test_transform)

# Create Validation Dataset
train_size = int(0.8 * len(train_dataset)) # 80% for training
val_size = len(train_dataset) - train_size # 20% for validation
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

# Update transformation for validation sets.
val_dataset.dataset.transform = val_test_transform

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=len(train_dataset) // 2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))
test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))

# Create CNN
class ResNet18(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.bn2 = nn.BatchNorm2d(64)
        self.bn3 = nn.BatchNorm2d(128)
        self.elu = nn.ELU()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(128*28*28, 256)
        self.fc2 = nn.Linear(256, 1)
        self.dropout = nn.Dropout(p=0.5)

    def _initialize_weights(self):
        for layer in self.children():
            if isinstance(layer, nn.Conv2d):
                init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='elu')  # Using ELU for activation
                if layer.bias is not None:
                    init.zeros_(layer.bias)  # Initialize biases to zero
            elif isinstance(layer, nn.Linear):
                init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='linear')  # For Linear layers
                init.zeros_(layer.bias)  # Initialize biases to zero

    def forward(self,x):
        x = self.pool(self.elu(self.bn1(self.conv1(x)))) # output dimension: 2x112x112
        x = self.pool(self.elu(self.bn2(self.conv2(x)))) # output dimension: 64x56x56
        x = self.pool(self.elu(self.bn3(self.conv3(x)))) # output dimension: 128x28x28
        x = self.flatten(x)
        x = self.elu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Training Loop
model = ResNet18()
model.train()
optimizer = optim.Adam(model.parameters(), lr = 0.0015, betas=(0.9,0.999))
criterion = nn.BCEWithLogitsLoss()
num_epoch = 35
loss_values = [] # for loss visualizing

for epoch in range(num_epoch):
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        output = model(inputs)
        loss = criterion(output, labels.float().view(-1,1))
        loss.backward()
        optimizer.step()

        # Update Accumulative Loss
        running_loss += loss.item()

    # Log Loss for visualization
    loss_values.append(loss.item())

    # Print Average Loss for each epoch
    epoch_loss = running_loss / len(train_loader)
    print(f'epoch:{epoch+1} / {num_epoch}, loss:{epoch_loss:.4f}')

# Visualize loss in training loop
plt.plot(loss_values)
plt.title('Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

"""### Below is the provided model evaluation code. Run the below cell to help you evaluate the accuracy and F1-score of your fine-tuned model."""

def evaluate_model(model, data_loader):
    model.eval()  # Set the model to evaluation mode

    # Initialize metrics
    accuracy_metric = Accuracy(task="binary")
    recall_metric = Recall(task="binary")
    f1_metric = F1Score(task="binary")
    precision_metric = Precision(task="binary")

    # Create lists to store all predictions and labels
    all_preds = []
    all_labels = []

    # Disable gradient calculation for evaluation
    with torch.no_grad():
        for inputs, labels in data_loader:
            outputs = model(inputs)
            preds = torch.sigmoid(outputs).round()  # Round to 0 or 1

            # Extend the lists with predictions and labels
            all_preds.extend(preds.tolist())
            all_labels.extend(labels.tolist())

    # Convert lists back to tensors after the loop
    all_preds = torch.tensor(all_preds)
    all_labels = torch.tensor(all_labels).view(-1,1)

    # Calculate performance metrics
    acc = accuracy_metric(all_preds, all_labels).item()
    recall = recall_metric(all_preds, all_labels).item()
    precision = precision_metric(all_preds, all_labels).item()
    f1 = f1_metric(all_preds, all_labels).item()

    # Return the metrics
    return acc, recall, precision, f1

# Evaluate on validation set.
val_accuracy, val_recall, val_precision, val_f1 = evaluate_model(model, val_loader)
print(f'Validation Accuracy: {val_accuracy:.4f}')
print(f'Validation Recall: {val_recall:.4f}')
print(f'Validation Precision: {val_precision:.4f}')
print(f'Validation F1 Score: {val_f1:.4f}')

# Save parameters from model trained
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    # You can also save epoch and loss if needed
    'epoch': epoch,
    'loss': loss,
}, "/content/drive/MyDrive/Colab Notebooks/CNN-Chest_X_Ray/20241103_model_checkpoint.pth")

'''
Validation Accuracy: 0.9167
Validation Recall: 0.8889
Validation Precision: 0.9231
Validation F1 Score: 0.9057
'''

# Later use on test set after completing hyperparameter tuning.
'''
test_accuracy, test_recall, test_precision, test_f1 = evaluate_model(model, test_loader)
print(f'Test Accuracy: {test_accuracy:.4f}')
print(f'Test Recall: {test_recall:.4f}')
print(f'Test Precision: {test_precision:.4f}')
print(f'Test F1 Score: {test_f1:.4f}')
'''